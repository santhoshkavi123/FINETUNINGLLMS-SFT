{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e532512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# load huggingface token\n",
    "from getpass import getpass\n",
    "os.environ[\"HF_TOKEN\"] = getpass(\"Enter the huggingface token: \")\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                         AutoModelForCausalLM,\n",
    "                         AutoConfig)\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b88321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0a87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name Tesla T4\n",
      "GPU version 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU version\", torch.version.cuda)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94610fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 14.74 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Reserved Memory: 0.00 GB\n",
      "Free (within reserved): 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def memory_info():\n",
    "    \" Function to check the total memory available\"\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory # Total memory of the GPUs\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) # currently allocated memory by tensors\n",
    "    reserved_memory = torch.cuda.memory_reserved(device) # memory reserved by the caching allocator \n",
    "\n",
    "    # Free memory (within reserved)\n",
    "    free_mem = reserved_memory - allocated_memory\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory/ 1024**3:.2f} GB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Reserved Memory: {reserved_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Free (within reserved): {free_mem / 1024**3:.2f} GB\")\n",
    "\n",
    "memory_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57efe2",
   "metadata": {},
   "source": [
    "# 1. Load the model to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fda810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'SanKav123/TinyBertModel',\n",
    "    dtype = torch.bfloat16, \n",
    "    use_cache = False\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1e896",
   "metadata": {},
   "source": [
    "# 2. Load Dataset\n",
    "\n",
    "Here we will update two methods on the Dataset object to allow it to interface with the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e512f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, split=\"train\"):\n",
    "        self.args = args\n",
    "        self.dataset = load_dataset(\n",
    "            args.dataset_name,\n",
    "            split = split\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Returns the number of samples in the datasets.\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"Retrieves a single data sample from the dataset at the specified index\"\n",
    "        input_ids = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "        labels = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "\n",
    "        # Return the sample as a dictionary\n",
    "        return {\"input_ids\":input_ids, \"labels\":labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c140ad8",
   "metadata": {},
   "source": [
    "# 3. Configure Training Arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a54d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import transformers\n",
    "\n",
    "@dataclass\n",
    "class CustomArguments(transformers.TrainingArguments):\n",
    "    dataset_name: str = field(\n",
    "        default = 'SanKav123/packed_dataset.parquet' # Dataset configuration\n",
    "    )\n",
    "    num_proc: int = field(default = 1) # Number of subprocesses for data preprocessing\n",
    "    max_seq_length: int = field(default = 32) # Maximum sequence length\n",
    "\n",
    "    # Core training configuration\n",
    "    seed: int = field(default = 0) # Random seed for initializing, ensuring reproduction\n",
    "    optim: str = field(default = \"adamw_torch\") # Optimizer, here it's a AdamW implemented by pytorch\n",
    "    max_steps: int = field(default = 200) # Number of maximum training steps\n",
    "    per_device_train_batch_size: int = field(default = 2) # Batch size per device during training\n",
    "\n",
    "    # Other training configuration\n",
    "    learning_rate: float = field(default = 5e-5) # Initial learning rate for the optimizer\n",
    "    weight_decay: float = field(default = 0)  # Weight decay\n",
    "    warmup_steps: int = field(default = 10) # Number of steps for the learning rate warmup phase\n",
    "    lr_scheduler_type: str = field(default = \"linear\") # Type of learning rate scheduler\n",
    "    gradient_checkpointing: bool = field(default = True) # Enabling gradient checkpointing to save memory\n",
    "    dataloader_num_workers: int = field(default = 2) # Number of subprocesses for data loading\n",
    "    bf16: bool = field(default = True) # Use bfloat16 precision for training on supported hardware\n",
    "    gradient_accumulation_steps: int = field(default = 1) # Number of steps to accumulate gradients before updating\n",
    "    \n",
    "\n",
    "    # logging configuration\n",
    "    logging_steps: int = field(default = 3) # Frequency of logging training information\n",
    "    report_to: str = field(default = \"none\") # Destination for logging (e.g.., WandB, TensorBoard)\n",
    "\n",
    "    # Saving configuration\n",
    "    # save_strategy: str = field(default=\"steps\")          # Can be replaced with \"epoch\"\n",
    "    # save_steps: int = field(default=3)                   # Frequency of saving training checkpoint\n",
    "    # save_total_limit: int = field(default=2)             # The total number of checkpoints to be saved\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca3c896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the custom arguments and set the output directory where the model will be saved\n",
    "\n",
    "parser = transformers.HfArgumentParser(CustomArguments)\n",
    "args, = parser.parse_args_into_dataclasses(\n",
    "    args = [\"--output_dir\", \"output\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4543e1",
   "metadata": {},
   "source": [
    "# 4. Setup the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c79a4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4903c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Check the shape fo the dataset\n",
    "print(\"Input Shape: \", train_dataset[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e0419",
   "metadata": {},
   "source": [
    "# 5. Run The trainer and monitor the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "491e4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "# Define a custom callback to log the loss value\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs = None, **kwargs):\n",
    "        if logs is not None:\n",
    "            self.logs.append(logs)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "# Initialize the callback\n",
    "loss_logging_callback = LossLoggingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f6ce067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>7.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>7.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>7.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.896600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>8.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>5.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>7.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>7.790900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>6.740900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>7.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>8.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>6.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>6.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>7.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>6.817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>6.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>6.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>6.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>6.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>5.919800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>6.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>5.765100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>6.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>6.772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>6.937600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>5.370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>7.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>6.987700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>6.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>6.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>7.196200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>5.882200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>6.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>6.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>6.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>6.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>6.701400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>5.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>6.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>7.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>6.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>6.773100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>6.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>5.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>7.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>5.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>6.883900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>6.869100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>5.746400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>6.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>6.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>5.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>6.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>6.633700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=6.521214308738709, metrics={'train_runtime': 42.289, 'train_samples_per_second': 9.459, 'train_steps_per_second': 4.729, 'total_flos': 21202285363200.0, 'train_loss': 6.521214308738709, 'epoch': 0.06668889629876626})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we can create an instance of the HuggingFace Trainer object from the transformers library. Call the train() method of the trainder to initialize the trainer run:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = pretrained_model, \n",
    "    args = args, \n",
    "    train_dataset = train_dataset, \n",
    "    eval_dataset = None, \n",
    "    callbacks = [loss_logging_callback]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e820b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "RepoUrl('https://huggingface.co/SanKav123/output', endpoint='https://huggingface.co', repo_type='model', repo_id='SanKav123/output')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "# HfApi().delete_repo(\"SanKav123/PreTrainedModel\")\n",
    "\n",
    "from huggingface_hub import create_repo\n",
    "create_repo(repo_id=\"SanKav123/output\", repo_type=\"model\", private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9aaae90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\n",
      "Configuration saved in output/config.json\n",
      "Configuration saved in output/generation_config.json\n",
      "Model weights saved in output/model.safetensors\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9a19d4aca242f1987b94dc791f8656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756abd4fa72f4fa0b801eae4f1741faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ea5a8cb2bb4e199875fb947efdb710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  .../output/training_args.bin: 100%|##########| 5.84kB / 5.84kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962e8046de1a4f65a57a9c68dfac5580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  .../output/model.safetensors:   7%|6         | 41.9MB /  618MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SanKav123/output/commit/37ce4836248382eac7bcb4c7b92318c05a0a5b69', commit_message='End of training', commit_description='', oid='37ce4836248382eac7bcb4c7b92318c05a0a5b69', pr_url=None, repo_url=RepoUrl('https://huggingface.co/SanKav123/output', endpoint='https://huggingface.co', repo_type='model', repo_id='SanKav123/output'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42c93243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1effeb94d93c40959cf1eff29584228a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ecff7855cc413397af0c76705cad18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e414af730943939a16c6d67e0fa1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166c08072a6445f1ac921f61c4284a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100, vocab={\"[PAD]\":0, \"[unused0]\":1, \"[unused1]\":2, \"[unused2]\":3, \"[unused3]\":4, ...})\n",
      "BOS token ID: 30522\n",
      "EOS token ID: 30523\n"
     ]
    }
   ],
   "source": [
    "# We will be using wordpiece which resembles BPE (Bit Pair Encoding)\n",
    "model_path_or_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                                        model_path_or_name,\n",
    "                                        bos_token = \"[BOS]\", # Define the BOS token string\n",
    "                                        eos_token = \"[EOS]\", # Define the EOS token string\n",
    "                                        use_fast = True\n",
    "                                    )\n",
    "print(tokenizer._tokenizer.model)\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73302e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2e81a995a343c1aee8763f83e3bdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SanKav123--output/snapshots/37ce4836248382eac7bcb4c7b92318c05a0a5b69/config.json\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b20aafcdb74ab0af0a6b5b50c6160c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/618M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--SanKav123--output/snapshots/37ce4836248382eac7bcb4c7b92318c05a0a5b69/model.safetensors\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e298e84842426a8ba51a0049069b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/133 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--SanKav123--output/snapshots/37ce4836248382eac7bcb4c7b92318c05a0a5b69/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside SanKav123/output.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"SanKav123/output\"\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27159b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2572, 2019, 3992, 1012, 1045, 2293,  102]],\n",
       "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dbecb626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ). you = you is a $ ) $, \\ frac { ^ and - 5 -. i was. are n ( is of the. that,, \\ frac { ^ -. is jacobite. that that $ [unused573] 2 ) 1 $ $ ) $ $ $ $ $ \\ frac {\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you talk about and operation\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model2.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=64,     \n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49ddc9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "tokens = model2.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6fd7ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'can',\n",
       " 'you',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'and',\n",
       " 'operation',\n",
       " '[SEP]',\n",
       " 'of',\n",
       " 'the',\n",
       " ',',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb63e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not that great results we have to keep training and process the data a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93257060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
