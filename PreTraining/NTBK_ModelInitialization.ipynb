{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0589de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          AutoConfig)\n",
    "from transformers import LlamaConfig\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import TextStreamer\n",
    "\n",
    "import torch \n",
    "def fix_torch_seed(seed = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc275a",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "be3f1f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_large_number(n):\n",
    "    \"\"\"\n",
    "    Formats a large number into a human-readable string (e.g., 1.2M, 3.4B).\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return '0'\n",
    "    \n",
    "    n = float(n)\n",
    "    # Define magnitude suffixes\n",
    "    magnitudes = ['', ' Thousand', ' M', ' B', ' T']\n",
    "    # Calculate the index of the appropriate magnitude\n",
    "    millidx = max(0, min(len(magnitudes) - 1, int(math.floor(math.log10(abs(n)) / 3))))\n",
    "    \n",
    "    # Format the number to one decimal place and append the suffix\n",
    "    return '{:.1f}{}'.format(n / 10**(3 * millidx), magnitudes[millidx]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ca1098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = getpass(\"Enter API Key : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "312e53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: Tesla T4\n",
      "GPU Version: 12.6\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Version: {torch.version.cuda}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11c954",
   "metadata": {},
   "source": [
    "# 1. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec04d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b839150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, update parameters to change the model architecture\n",
    "config.num_hidden_layers = 12\n",
    "config.hidden_size = 1024\n",
    "config.intermediate_size = 4096\n",
    "config.num_key_value_heads = 8\n",
    "config.torch_dtype = \"bfloat16\"\n",
    "config.use_cache = False\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919e44d",
   "metadata": {},
   "source": [
    "# 2 Weight Initialization\n",
    "\n",
    "We have different ways of initializing weights of a model for training\n",
    "\n",
    "- Random Weight Initialization\n",
    "- Using an existing model for continued pre-training\n",
    "- Downscaling an existing model\n",
    "- Upscaling an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffc310",
   "metadata": {},
   "source": [
    "## 2.1 Random Weight Initialization \n",
    "\n",
    "Randomly initializing model weights sets all weights to values from a truncated normal distribution with mean 0 and standard deviation 0.02. Values beyond 2-sigma from the mean are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea244ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM(config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da70902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is : 342385664\n"
     ]
    }
   ],
   "source": [
    "# \"numel\" this function calculates the product of all dimensions of a tensor, effectively providing its total size regardless of its shape or dimensionality'\n",
    "def print_nparams(model):\n",
    "    total_params = 0\n",
    "    for p in model.parameters():\n",
    "        total_params += p.numel()\n",
    "    \n",
    "    print(f\"The total number of parameters is : {total_params}\")\n",
    "\n",
    "\n",
    "print_nparams(model)\n",
    "# We see that's a lot parameters to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ा epleepleep alk alkleepleepleep alk alk alk teams });teamsweetााााा Items Items Items Items Items Items Itemsाा Items Items Itemsाााा Itemsा Items Items Items Items Items Items Items Itemsाााााा Items Items Items Items Items Items Items Items Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## назна## назна## назна назна назна назна назна назна назна\n"
     ]
    }
   ],
   "source": [
    "# First we will load the pretrained Llama Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"huggyllama/llama-7b\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(\"What is the most complicated maths topics?\", \n",
    "                        return_tensors = \"pt\").to(device)\n",
    "\n",
    "# Streamer \n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt = True, \n",
    "    skip_special_tokens = True\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the output from the model\n",
    "outputs = model.generate(\n",
    "    **input_ids, \n",
    "    streamer = streamer, \n",
    "    use_cache = True, \n",
    "    max_new_tokens = 128, \n",
    "    do_sample = False\n",
    ")\n",
    "\n",
    "# Some gubrish has generated the output, through this we can understand that we have to training the model for longer time to get the better language understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c52a485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41129eb0",
   "metadata": {},
   "source": [
    "## 2.2Reuse general pretrained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b13ba65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f493eba72014093a02a33d9d512a797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm not a mathematician, but I'm interested in maths. I'm interested in the most complicated maths.\n",
      "I'm not interested in the most complicated maths that is used in everyday life, like the maths used in a computer. I'm interested in the most complicated maths that is used "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 6440 has 14.74 GiB memory in use. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 754.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2078795978.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m outputs = model.generate(**input_ids, \n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mstreamer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;31m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mcache_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"sin\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cos\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cache_position\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mattention_interface\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meager_attention_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_non_sliding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffloading\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, key_states, value_states, cache_kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_initialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 6440 has 14.74 GiB memory in use. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 754.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"huggyllama/llama-7b\", \n",
    "    dtype = torch.bfloat16, \n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"huggyllama/llama-7b\"\n",
    ")\n",
    "\n",
    "# Input Ids to the tokenizer\n",
    "input_ids = tokenizer(\"What are the most complicated maths?\", return_tensors = \"pt\").to(device)\n",
    "\n",
    "# Define Streamer \n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt = True, \n",
    "    skip_special_tokens = True\n",
    ")\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "                streamer = streamer, \n",
    "                use_cache= True, \n",
    "                max_new_tokens = 128,\n",
    "                do_sample = False)\n",
    "\n",
    "# we can see the model is clearly with the english language, it can generate the content related to the question."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "59d473d1",
   "metadata": {},
   "source": [
    "We have exhausted all the memory by loading the 7b parameters model\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5131"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3405d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 14.74 GB\n",
      "Allocated GPU Memory: 0.01 GB\n",
      "Reserved Memory: 14.61 GB\n",
      "Free (within reserved): 14.61 GB\n"
     ]
    }
   ],
   "source": [
    "def memory_info():\n",
    "    \" Function to check the total memory available\"\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory # Total memory of the GPUs\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) # currently allocated memory by tensors\n",
    "    reserved_memory = torch.cuda.memory_reserved(device) # memory reserved by the caching allocator \n",
    "\n",
    "    # Free memory (within reserved)\n",
    "    free_mem = reserved_memory - allocated_memory\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory/ 1024**3:.2f} GB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Reserved Memory: {reserved_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Free (within reserved): {free_mem / 1024**3:.2f} GB\")\n",
    "\n",
    "memory_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c13a5",
   "metadata": {},
   "source": [
    "## 2.3 Downscaling from a general pretrained model\n",
    "\n",
    "Downscaling the model we are gonna few layers. Let's see how we are gonna drop few of the layers from the original model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0079a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load a one billion llama model\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/llama-3.2-1B',\n",
    "                                            device_map = \"auto\",\n",
    "                                            torch_dtype = \"bfloat16\"\n",
    "                                            )                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d3b4c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The answer is: it depends. The most complicated maths is the maths that is used to describe the most complicated things. The most complicated maths is the maths that is used to describe the most complicated things. The most complicated maths is the maths that is used to describe the most complicated things. The most complicated maths is the maths that is used to describe the most complicated things. The most complicated maths is the maths that is used to describe the most complicated things. The most complicated maths is the maths that is used to describe the most complicated things. The most complicated maths is the maths that is used to describe the most complicated things. The most\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/llama-3.2-1B\"\n",
    ")\n",
    "\n",
    "# Input Ids to the tokenizer\n",
    "input_ids = tokenizer(\"What is the most complicated maths?\", return_tensors = \"pt\").to(device)\n",
    "\n",
    "# Define Streamer \n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt = True, \n",
    "    skip_special_tokens = True\n",
    ")\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "                streamer = streamer, \n",
    "                use_cache= True, \n",
    "                max_new_tokens = 128,\n",
    "                do_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "37dc9635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "The total number of parameters is : 1235814400\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "print_nparams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df638c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 14.74 GB\n",
      "Allocated GPU Memory: 4.61 GB\n",
      "Reserved Memory: 14.61 GB\n",
      "Free (within reserved): 10.00 GB\n"
     ]
    }
   ],
   "source": [
    "memory_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463ebe7",
   "metadata": {},
   "source": [
    "After downscaling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e38d0d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = model.model.layers\n",
    "\n",
    "# We are having 16 llamaDecoder layers, we are using only 10 layers for pretraining, first 5 and last 5 layers\n",
    "model.model.layers = layers[:5] + layers[-5:]\n",
    "\n",
    "# Updating the config \n",
    "config = AutoConfig.from_pretrained(\n",
    "    'meta-llama/llama-3.2-1B', \n",
    "    num_hidden_layers = len(model.model.layers)\n",
    ")\n",
    "\n",
    "# set the new config to the model \n",
    "model.config = config\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "de2c1a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is : 870885376\n",
      "The number of parameters excluded during downscaling : 364.9 M\n"
     ]
    }
   ],
   "source": [
    "# Downscaling model : num of params\n",
    "print_nparams(model)\n",
    "\n",
    "print(f\"The number of parameters excluded during downscaling : {format_large_number(1235814400-870885376)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde5a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The video is being, fact to get, not the right on the right the 1. 5. The 5-1, the 1. 5. 1. 6. and 5. and 1. 7. 1. 1. 1. 3. 3, the 1s 1 1 is a factor of 1 and 1 will I and 1 will 1 I, 1 1 I, 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/llama-3.2-1B\"\n",
    ")\n",
    "\n",
    "# Input Ids to the tokenizer\n",
    "input_ids = tokenizer(\"What is the most complicated maths?\", return_tensors = \"pt\").to(device)\n",
    "\n",
    "# Define Streamer \n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt = True, \n",
    "    skip_special_tokens = True\n",
    ")\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "                streamer = streamer, \n",
    "                use_cache= False, # make sure to set this as False because we are using the downscale model and it will throw error if we set it to True\n",
    "                max_new_tokens = 128,\n",
    "                do_sample = False)\n",
    "\n",
    "# Again we are getting the output, but it doesn't make any sense, so training is required for the model to make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1f9a044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 14.74 GB\n",
      "Allocated GPU Memory: 4.61 GB\n",
      "Reserved Memory: 14.61 GB\n",
      "Free (within reserved): 10.00 GB\n"
     ]
    }
   ],
   "source": [
    "memory_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bebb5c6",
   "metadata": {},
   "source": [
    "## 2.4 Upscaling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6a82a",
   "metadata": {},
   "source": [
    "Here, we are going to upscale the llama 16 layers to 20 layers. Here are the steps we are going to follow\n",
    "\n",
    "1. Configure a 20 layer model and initialize it with random weights\n",
    "2. Load the 16 layer model into memory_info\n",
    "3. Copy the bottom 10 layers and top 10 layers from the 16 layer model and use them to overwrite the random weights of the 20 layer model\n",
    "4. Copy over the embedding and classifying layers to replace the randomly initialized counterparts in the 20 layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "39cdd089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_val_heads\": 8,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    num_hidden_layers = 16, \n",
    "    hidden_size = 2048, \n",
    "    intermediate_size = 8192,\n",
    "    num_attention_heads = 32, \n",
    "    num_key_val_heads = 8,\n",
    "    torch_dtype = \"bfloat16\", \n",
    "    use_cache = False\n",
    ")\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "61eb24b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n",
      "The total number of parameters is : 1204881408\n"
     ]
    }
   ],
   "source": [
    "# Step -1 Configure over 20 layer model with randomly initialized weights\n",
    "model = LlamaForCausalLM(config)\n",
    "model = model.to(dtype = torch.bfloat16)\n",
    "print(model)\n",
    "print_nparams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c6dd56da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is : 1235814400\n"
     ]
    }
   ],
   "source": [
    "# load pretrained llama -1B million\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained('meta-llama/llama-3.2-1B', \n",
    "                                        device_map= \"auto\", \n",
    "                                        torch_dtype = torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/llama-3.2-1B')\n",
    "\n",
    "print_nparams(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "09047cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_val_heads\": 8,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, we copy the weights from the pretrained model and overwrite the randomly generated weight in our custom configuration\n",
    "from copy import deepcopy\n",
    "\n",
    "model.model.layers = deepcopy(pretrained_model.model.layers[:-6]) + deepcopy(pretrained_model.model.layers[6:])\n",
    "\n",
    "# Embedding layer\n",
    "model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n",
    "\n",
    "# lm head\n",
    "model.lm_head = deepcopy(pretrained_model.lm_head)\n",
    "\n",
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "299a9376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is : 1741768704\n",
      "formatted number : 1.7 B\n"
     ]
    }
   ],
   "source": [
    "print_nparams(model)\n",
    "print(\"formatted number :\", format_large_number(1741767680))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "71e24d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-19): 20 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "aa9887d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths maths­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i­i\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/llama-3.2-1B\"\n",
    ")\n",
    "\n",
    "# Input Ids to the tokenizer\n",
    "input_ids = tokenizer(\"What is the most complicated maths?\", return_tensors = \"pt\").to(device)\n",
    "\n",
    "# Define Streamer \n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt = True, \n",
    "    skip_special_tokens = True\n",
    ")\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "                streamer = streamer, \n",
    "                use_cache= False, # make sure to set this as False because we are using the downscale model and it will throw error if we set it to True\n",
    "                max_new_tokens = 128,\n",
    "                do_sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d72b8",
   "metadata": {},
   "source": [
    "# Save this model to the HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"./PreTraining/llama_depth_upscaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b44a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
