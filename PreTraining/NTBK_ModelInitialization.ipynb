{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0589de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer)\n",
    "from transformers import LlamaConfig\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import TextStreamer\n",
    "\n",
    "import torch \n",
    "def fix_torch_seed(seed = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "312e53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: Tesla T4\n",
      "GPU Version: 12.6\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Version: {torch.version.cuda}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11c954",
   "metadata": {},
   "source": [
    "# 1. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec04d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b839150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, update parameters to change the model architecture\n",
    "config.num_hidden_layers = 12\n",
    "config.hidden_size = 1024\n",
    "config.intermediate_size = 4096\n",
    "config.num_key_value_heads = 8\n",
    "config.torch_dtype = \"bfloat16\"\n",
    "config.use_cache = False\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919e44d",
   "metadata": {},
   "source": [
    "# 2 Weight Initialization\n",
    "\n",
    "We have different ways of initializing weights of a model for training\n",
    "\n",
    "- Random Weight Initialization\n",
    "- Using an existing model for continued pre-training\n",
    "- Downscaling an existing model\n",
    "- Upscaling an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffc310",
   "metadata": {},
   "source": [
    "# Random Weight Initialization \n",
    "\n",
    "Randomly initializing model weights sets all weights to values from a truncated normal distribution with mean 0 and standard deviation 0.02. Values beyond 2-sigma from the mean are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea244ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM(config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1da70902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is : 342385664\n"
     ]
    }
   ],
   "source": [
    "# \"numel\" this function calculates the product of all dimensions of a tensor, effectively providing its total size regardless of its shape or dimensionality'\n",
    "def print_nparams(model):\n",
    "    total_params = 0\n",
    "    for p in model.parameters():\n",
    "        total_params += p.numel()\n",
    "    \n",
    "    print(f\"The total number of parameters is : {total_params}\")\n",
    "\n",
    "\n",
    "print_nparams(model)\n",
    "# We see that's a lot parameters to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ा epleepleep alk alkleepleepleep alk alk alk teams });teamsweetााााा Items Items Items Items Items Items Itemsाा Items Items Itemsाााा Itemsा Items Items Items Items Items Items Items Itemsाााााा Items Items Items Items Items Items Items Items Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musicalionali Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## Musical## назна## назна## назна назна назна назна назна назна назна\n"
     ]
    }
   ],
   "source": [
    "# First we will load the pretrained Llama Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"huggyllama/llama-7b\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(\"What is the most complicated maths topics?\", \n",
    "                        return_tensors = \"pt\").to(device)\n",
    "\n",
    "# Streamer \n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt = True, \n",
    "    skip_special_tokens = True\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the output from the model\n",
    "outputs = model.generate(\n",
    "    **input_ids, \n",
    "    streamer = streamer, \n",
    "    use_cache = True, \n",
    "    max_new_tokens = 128, \n",
    "    do_sample = False\n",
    ")\n",
    "\n",
    "# Some gubrish has generated the output, through this we can understand that we have to training the model for longer time to get the better language understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52a485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
